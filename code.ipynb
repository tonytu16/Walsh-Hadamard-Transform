{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wht_spectrum(model, H, all_inputs, device=\"cuda\"):\n",
    "    '''\n",
    "    Compute the WH spectrum of the DNN model\n",
    "    Used for EN-regularization\n",
    "    '''\n",
    "    all_inputs = all_inputs.to(device)\n",
    "    all_inputs = all_inputs.float()\n",
    "    H = H.to(device)\n",
    "    # get all outputs of model (aka landscape)\n",
    "    landscape = model(all_inputs).reshape(-1)\n",
    "    # compute spectrum by matrix-vector multiplication with WHT basis\n",
    "    landscape = landscape.double()\n",
    "    spectrum = torch.matmul(H, landscape)\n",
    "    # clear up gpu memory\n",
    "    H.to(\"cpu\")\n",
    "    all_inputs.to(\"cpu\")\n",
    "    return spectrum\n",
    "\n",
    "def walsh_hadamard_matrix(L=13, normalize=False):\n",
    "    '''\n",
    "    Compute the WHT matrix for domain of dimension L\n",
    "    '''\n",
    "    H1 = np.asarray([[1.,1.], [1.,-1.]])\n",
    "    H = np.asarray([1.])\n",
    "    for i in range(L):\n",
    "        H = np.kron(H, H1)\n",
    "    if normalize:\n",
    "        H = (1 / np.sqrt(2**L)) * H\n",
    "    return H\n",
    "ratio_matrices = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_fc_2(nn.Module):\n",
    "    def __init__(self, n, multiplier):\n",
    "        super(Net_fc_2, self).__init__()\n",
    "        self.fc1 = nn.Linear(n, multiplier*n)\n",
    "        self.bn1 = nn.BatchNorm1d(multiplier*n)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(multiplier*n, 1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(F.leaky_relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x.reshape(-1)\n",
    "    \n",
    "class ConvNeuralNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNeuralNet2, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels=13, out_channels=13, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(1, 1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(13)\n",
    "        self.conv2 = torch.nn.Conv1d(in_channels=13, out_channels=1, kernel_size=3, padding=1)\n",
    "        self.fc2 = nn.Linear(1, 1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm1(F.leaky_relu(self.fc1(self.conv1(x))))\n",
    "        x = self.fc2(F.leaky_relu(self.conv2(x)))\n",
    "        return x.reshape(-1)\n",
    "    \n",
    "class ConvNeuralNet4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNeuralNet4, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(in_channels=13, out_channels=13, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(1, 1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(13)\n",
    "        self.conv2 = torch.nn.Conv1d(in_channels=13, out_channels=13, kernel_size=3, padding=1)\n",
    "        self.fc2 = nn.Linear(1, 1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(13)\n",
    "        self.conv3 = torch.nn.Conv1d(in_channels=13, out_channels=10, kernel_size=3, padding=1)\n",
    "        self.fc3 = nn.Linear(1, 1)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(10)\n",
    "        self.conv4 = torch.nn.Conv1d(in_channels=10, out_channels=1, kernel_size=3, padding=1)\n",
    "        self.fc4 = nn.Linear(1, 1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc4.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv3.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.conv4.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm1(F.leaky_relu(self.fc1(self.conv1(x))))\n",
    "        x = self.batchnorm2(F.leaky_relu(self.fc2(self.conv2(x))))\n",
    "        x = self.batchnorm3(F.leaky_relu(self.fc3(self.conv3(x))))\n",
    "        x = self.fc4(F.leaky_relu(self.conv4(x)))\n",
    "        return x.reshape(-1)\n",
    "    \n",
    "class Net_fc_4(nn.Module):\n",
    "    def __init__(self, n, multiplier):\n",
    "        super(Net_fc_4, self).__init__()\n",
    "        self.fc1 = nn.Linear(n, multiplier*n)\n",
    "        self.bn1 = nn.BatchNorm1d(multiplier*n)\n",
    "        self.fc2 = nn.Linear(multiplier*n, multiplier*n)\n",
    "        self.bn2 = nn.BatchNorm1d(multiplier*n)\n",
    "        self.fc3 = nn.Linear(multiplier*n, n)\n",
    "        self.bn3 = nn.BatchNorm1d(n)\n",
    "        self.fc4 = nn.Linear(n, 1)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(F.leaky_relu(self.fc1(x)))\n",
    "        x = self.bn2(F.leaky_relu(self.fc2(x)))\n",
    "        x = self.bn3(F.leaky_relu(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        return x.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, val_dl, H, all_inputs, lr=1e-1, weight_decay=0.0, reg_lambda=1e-3, num_epochs=200, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    for epoch in range(num_epochs):\n",
    "        ###############################\n",
    "        ########## TRAIN LOOP #########\n",
    "        ###############################\n",
    "        print('training epoch ' + str(epoch))\n",
    "        if epoch >= 0:\n",
    "            #modify model path\n",
    "            path = \"testing/models/fc_net_reg_xavier/\" + \"2_layer_fc_net_epoch_\" + str(epoch) + \".pt\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_State_dict': optim.state_dict(),\n",
    "            }, path)\n",
    "        model.train()\n",
    "        for X, y in train_dl:\n",
    "            optim.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            X = X.float()\n",
    "            y = y.float()\n",
    "            y_hat = model(X)\n",
    "            loss = F.mse_loss(y, y_hat)\n",
    "            #Apply EN-regularization\n",
    "            spectrum = compute_wht_spectrum(model, H, all_inputs, device=device)\n",
    "            reg_loss = F.l1_loss(spectrum, torch.zeros_like(spectrum))\n",
    "            loss += reg_lambda * reg_loss\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        ###############################\n",
    "        ########## VAL LOOP #########\n",
    "        ###############################\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X, y = next(iter(train_dl))\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            X = X.float()\n",
    "            y = y.float()\n",
    "            #add these lines for CNN model training\n",
    "#             X = X.view([X.shape[0], X.shape[1], 1, 1])\n",
    "#             y = y.view([y.shape[0], 1, 1, 1])\n",
    "            y_hat = model(X)\n",
    "            val_loss = F.mse_loss(y, y_hat).item()\n",
    "            print(f\"Validation Loss: {val_loss:.3f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_name):\n",
    "    ratio_matrix = np.zeros((100,298))\n",
    "    sigma_vec = np.linspace(0.03, 3, 298)\n",
    "    for sigma_ind,sigma_val in enumerate(sigma_vec):\n",
    "        print('sigma=',sigma_val)\n",
    "        for repeat in range(100):\n",
    "            random.seed(4)\n",
    "            np.random.seed(4)\n",
    "            torch.manual_seed(4)\n",
    "            \n",
    "            model = Net_fc_2(L, 1)\n",
    "            optim = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "            #load model here and run this function everytime we need to evaluate a new model\n",
    "            PATH = \"testing/models/fc_net_reg_xavier/\" + model_name\n",
    "            checkpoint = torch.load(PATH)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optim.load_state_dict(checkpoint['optimizer_State_dict'])\n",
    "            epoch = checkpoint['epoch']\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                X, y = next(iter(train_dl))\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                X = X.float()\n",
    "                y = y.float()\n",
    "                y_hat_start = model(X)\n",
    "\n",
    "            seed_multiplier = 0\n",
    "            random.seed(repeat+1000000*seed_multiplier)\n",
    "            np.random.seed(repeat+1000000*seed_multiplier)\n",
    "            torch.manual_seed(repeat+1000000*seed_multiplier)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for param in model.parameters():\n",
    "                    added_noise = torch.randn(param.size()) * sigma_val\n",
    "                    param.add_(added_noise)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                X, y = next(iter(train_dl))\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                X = X.float()\n",
    "                y = y.float()\n",
    "                y_hat = model(X)\n",
    "                test_loss = F.mse_loss(y_hat, y_hat_start).item()\n",
    "                actual_test_loss = F.mse_loss(y, y_hat_start).item()\n",
    "                ratio_matrix[repeat,sigma_ind]=test_loss\n",
    "    return ratio_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 13\n",
    "N = 2**L\n",
    "H = walsh_hadamard_matrix(L=L)\n",
    "all_inputs = np.asarray(list((product((0,1), repeat=L))))\n",
    "H = torch.tensor(H, dtype=torch.float)\n",
    "all_inputs = torch.tensor(all_inputs, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "###### Get Dataset and Split Train/Test ########\n",
    "################################################\n",
    "def generate_training_data(num_examples, extra=None):\n",
    "    polynomial = '3*x_1 + 4*x_2*x_3 + 5*x_4*x_5 + x_12'\n",
    "    all_inputs = np.asarray(list((product((-1,1), repeat=L))))\n",
    "    all_outputs = []\n",
    "    for i in range(N):\n",
    "        x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_10, x_11, x_12, x_13 = all_inputs[i]\n",
    "        all_outputs.append(eval(polynomial))\n",
    "    all_inputs = torch.tensor(all_inputs)\n",
    "    all_outputs = torch.tensor(all_outputs)\n",
    "    return all_inputs, all_outputs\n",
    "\n",
    "X_all, y_all = generate_training_data(100, extra=None)\n",
    "#reshape data if training cnn \n",
    "# X_all = X_all.reshape([X_all.shape[0], X_all.shape[1], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "###### Get Dataset and Split Train/Test ########\n",
    "################################################\n",
    "\n",
    "torch.manual_seed(4)\n",
    "random.seed(4)\n",
    "np.random.seed(4)\n",
    "\n",
    "n_train = 1000\n",
    "bs = 16\n",
    "n = X_all.shape[0]\n",
    "ds = TensorDataset(X_all,y_all)\n",
    "# Create Datasets / DataLoaders\n",
    "train_ds, val_ds, test_ds = random_split(ds, lengths=[n_train, n_train, n - 2*n_train])\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=len(val_ds), shuffle=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=len(test_ds), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters for training\n",
    "lr = 5e-3\n",
    "weight_decay = 0.0\n",
    "reg_lambda = 1e-3  ## with EN regularization\n",
    "num_epochs = 100\n",
    "device = \"cpu\"\n",
    "H = torch.tensor(walsh_hadamard_matrix(L=L))\n",
    "\n",
    "torch.manual_seed(4)\n",
    "random.seed(4)\n",
    "np.random.seed(4)\n",
    "model = Net_fc_2(L, 1)\n",
    "model = train_model(model, train_dl, val_dl, H, X_all, \n",
    "                    lr=lr, weight_decay=weight_decay, reg_lambda=reg_lambda, \n",
    "                    num_epochs=num_epochs, device=device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X, y = next(iter(train_dl))\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    X = X.float()\n",
    "    y = y.float()\n",
    "    y_hat = model(X)\n",
    "    test_loss = F.mse_loss(y_hat, y).item()\n",
    "    print(f\"Test Loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "###### Get Dataset and Split Train/Test ########\n",
    "################################################\n",
    "# (for evaluation)\n",
    "\n",
    "torch.manual_seed(4)\n",
    "random.seed(4)\n",
    "np.random.seed(4)\n",
    "\n",
    "n_train = 1000\n",
    "bs = 1000\n",
    "n = X_all.shape[0]\n",
    "ds = TensorDataset(X_all,y_all)\n",
    "# Create Datasets / DataLoaders\n",
    "train_ds, val_ds, test_ds = random_split(ds, lengths=[n_train, n_train, n - 2*n_train])\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=len(val_ds), shuffle=False)\n",
    "test_dl = DataLoader(test_ds, batch_size=len(test_ds), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"2_layer_fc_net_epoch_0.pt\", \"2_layer_fc_net_epoch_1.pt\", \"2_layer_fc_net_epoch_2.pt\", \"2_layer_fc_net_epoch_3.pt\"]\n",
    "for model in models:\n",
    "    ratio_matrices.append(evaluate(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_vec = np.linspace(0.03, 3, 298)\n",
    "plt.loglog(np.multiply(sigma_vec,sigma_vec),np.divide(np.min(ratio_matrices[0],axis=0),np.multiply(sigma_vec,sigma_vec)),'o', markersize=3, label = '0')\n",
    "plt.loglog(np.multiply(sigma_vec,sigma_vec),np.divide(np.min(ratio_matrices[1],axis=0),np.multiply(sigma_vec,sigma_vec)),'o', markersize=3, label = '1')\n",
    "plt.loglog(np.multiply(sigma_vec,sigma_vec),np.divide(np.min(ratio_matrices[2],axis=0),np.multiply(sigma_vec,sigma_vec)),'o', markersize=3, label = '2')\n",
    "plt.loglog(np.multiply(sigma_vec,sigma_vec),np.divide(np.min(ratio_matrices[3],axis=0),np.multiply(sigma_vec,sigma_vec)),'o', markersize=3, label = '3')\n",
    "plt.ylabel(r'$\\min \\sum_i \\|g_{\\theta}(x_i)-g_{\\theta^*}(x_i)\\|_2^2 / \\sigma^2$')\n",
    "plt.legend(loc=\"lower left\", prop={'size': 10})\n",
    "plt.xlabel(r'$\\sigma^2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
